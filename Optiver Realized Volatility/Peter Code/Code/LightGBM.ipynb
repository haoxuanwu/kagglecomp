{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b262e588-f9ac-4fa2-8cfa-48c5606743cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import torch\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgbm\n",
    "from IPython.display import display\n",
    "\n",
    "from joblib import delayed, Parallel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}sec')\n",
    "    \n",
    "def print_trace(name: str = ''):\n",
    "    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea4684f-0dc4-4cd7-8be7-a17ddee3793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../Data'\n",
    "train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "train = train[train['stock_id'] <= 2]\n",
    "stock_ids = set(train['stock_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9183536-39a7-4f1d-ba70-fc289d5689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBlock(Enum):\n",
    "    TRAIN = 1\n",
    "    TEST = 2\n",
    "    BOTH = 3\n",
    "    \n",
    "def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(os.path.join(DATA_DIR, directory, f'stock_id={stock_id}'))\n",
    "\n",
    "def load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n",
    "    if block == DataBlock.TRAIN:\n",
    "        return load_stock_data(stock_id, f'{stem}_train.parquet')\n",
    "    elif block == DataBlock.TEST:\n",
    "        return load_stock_data(stock_id, f'{stem}_test.parquet')\n",
    "    else:\n",
    "        return pd.concat([\n",
    "            load_data(stock_id, stem, DataBlock.TRAIN),\n",
    "            load_data(stock_id, stem, DataBlock.TEST)\n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "def load_book(stock_id: int, block: DataBlock=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'book', block)\n",
    "\n",
    "\n",
    "def load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'trade', block)\n",
    "\n",
    "def calc_wap1(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "def make_book_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    book['wap2'] = calc_wap2(book)\n",
    "    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n",
    "\n",
    "    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n",
    "    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n",
    "    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n",
    "    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n",
    "    \n",
    "    features = {\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "    }\n",
    "    \n",
    "    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n",
    "    agg.columns = flatten_name('book', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "    \n",
    "    for time in [450, 300, 150]:\n",
    "        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'book_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "def make_trade_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    trade = load_trade(stock_id, block)\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n",
    "\n",
    "    features = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':['count'],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()\n",
    "    agg.columns = flatten_name('trade', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "        \n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "def make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n",
    "    time_ids = list(set(prices.index))\n",
    "\n",
    "    ticks = {}\n",
    "    for tid in time_ids:\n",
    "        try:\n",
    "            price_list = prices.loc[tid].values.flatten()\n",
    "            price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "            ticks[tid] = price_diff[0]\n",
    "        except Exception:\n",
    "            print_trace(f'tid={tid}')\n",
    "            ticks[tid] = np.nan\n",
    "        \n",
    "    dst = pd.DataFrame()\n",
    "    dst['time_id'] = np.unique(book['time_id'])\n",
    "    dst['stock_id'] = stock_id\n",
    "    dst['tick_size'] = dst['time_id'].map(ticks)\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "def make_features(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n",
    "        book = pd.concat(books)\n",
    "\n",
    "    with timer('trades'):\n",
    "        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n",
    "        trade = pd.concat(trades)\n",
    "\n",
    "    with timer('extra features'):\n",
    "        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n",
    "        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n",
    "        #df = make_extra_features(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_features_v2(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books(v2)'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n",
    "        book_v2 = pd.concat(books)\n",
    "\n",
    "    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e763504-67e4-4c17-81f5-6b24d5c68910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[books]  24.425sec\n",
      "[trades]  3.642sec\n",
      "[extra features]  0.075sec\n"
     ]
    }
   ],
   "source": [
    "df = make_features(train, DataBlock.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facd0585-8118-4087-b92e-a51ffc536bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = lgbm.LGBMRegressor()\n",
    "df['stock_id'] = df['stock_id'].astype(str)\n",
    "dummies = pd.get_dummies(df['stock_id'])\n",
    "dummies.index = df.index\n",
    "df_enriched = pd.concat([df, dummies], axis = 1)\n",
    "Y = df['target']\n",
    "X = df_enriched.drop(['target', 'stock_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b8fa4fd-2c58-4f47-930e-0a958270e95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book.wap1.sum</th>\n",
       "      <th>book.wap2.sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303.125061</td>\n",
       "      <td>303.105539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200.047768</td>\n",
       "      <td>200.041171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187.913849</td>\n",
       "      <td>187.939824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119.859781</td>\n",
       "      <td>119.835941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.932865</td>\n",
       "      <td>175.934256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11485</th>\n",
       "      <td>552.091492</td>\n",
       "      <td>552.099365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11486</th>\n",
       "      <td>485.168854</td>\n",
       "      <td>485.163422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11487</th>\n",
       "      <td>486.847137</td>\n",
       "      <td>486.845856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11488</th>\n",
       "      <td>475.116730</td>\n",
       "      <td>475.116455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11489</th>\n",
       "      <td>471.808990</td>\n",
       "      <td>471.816162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11490 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       book.wap1.sum  book.wap2.sum\n",
       "0         303.125061     303.105539\n",
       "1         200.047768     200.041171\n",
       "2         187.913849     187.939824\n",
       "3         119.859781     119.835941\n",
       "4         175.932865     175.934256\n",
       "...              ...            ...\n",
       "11485     552.091492     552.099365\n",
       "11486     485.168854     485.163422\n",
       "11487     486.847137     486.845856\n",
       "11488     475.116730     475.116455\n",
       "11489     471.808990     471.816162\n",
       "\n",
       "[11490 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['book.wap1.sum', 'book.wap2.sum']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345a94d4-430c-4b74-8299-5c639663ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e969e-727b-4bfe-b0a0-c6c41f911eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = lgbm.LGBMRegressor(n_estimators=20)\n",
    "eval_set = [(X_eval, y_eval)]\n",
    "\n",
    "reg.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"mape\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518c853-0756-4f66-b1d7-af7b982442d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
