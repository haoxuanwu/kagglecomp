{"cells":[{"cell_type":"code","execution_count":1,"id":"b262e588-f9ac-4fa2-8cfa-48c5606743cd","metadata":{"id":"b262e588-f9ac-4fa2-8cfa-48c5606743cd","executionInfo":{"status":"ok","timestamp":1657999346694,"user_tz":300,"elapsed":6868,"user":{"displayName":"Haoxuan [Peter] Wu","userId":"09135652591918716343"}}},"outputs":[],"source":["import gc\n","import glob\n","import os\n","import time\n","import traceback\n","from contextlib import contextmanager\n","from enum import Enum\n","from typing import Dict, List, Optional, Tuple\n","import torch\n","\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import lightgbm as lgbm\n","from IPython.display import display\n","\n","from joblib import delayed, Parallel\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.manifold import TSNE\n","from sklearn.model_selection import GroupKFold\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.preprocessing import minmax_scale\n","from tqdm import tqdm_notebook as tqdm\n","\n","@contextmanager\n","def timer(name: str):\n","    s = time.time()\n","    yield\n","    elapsed = time.time() - s\n","    print(f'[{name}] {elapsed: .3f}sec')\n","    \n","def print_trace(name: str = ''):\n","    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n","    print(traceback.format_exc())"]},{"cell_type":"code","execution_count":2,"id":"eea4684f-0dc4-4cd7-8be7-a17ddee3793c","metadata":{"id":"eea4684f-0dc4-4cd7-8be7-a17ddee3793c","executionInfo":{"status":"error","timestamp":1657999355649,"user_tz":300,"elapsed":754,"user":{"displayName":"Haoxuan [Peter] Wu","userId":"09135652591918716343"}},"outputId":"055bce90-b61f-48f8-a7c8-2ee95dc40cee","colab":{"base_uri":"https://localhost:8080/","height":389}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c2551cf7af05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stock_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstock_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stock_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/train.csv'"]}],"source":["DATA_DIR = '../Data'\n","train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n","train = train[train['stock_id'] <= 2]\n","stock_ids = set(train['stock_id'])"]},{"cell_type":"code","execution_count":null,"id":"d9183536-39a7-4f1d-ba70-fc289d5689e4","metadata":{"id":"d9183536-39a7-4f1d-ba70-fc289d5689e4"},"outputs":[],"source":["class DataBlock(Enum):\n","    TRAIN = 1\n","    TEST = 2\n","    BOTH = 3\n","    \n","def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n","    return pd.read_parquet(os.path.join(DATA_DIR, directory, f'stock_id={stock_id}'))\n","\n","def load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n","    if block == DataBlock.TRAIN:\n","        return load_stock_data(stock_id, f'{stem}_train.parquet')\n","    elif block == DataBlock.TEST:\n","        return load_stock_data(stock_id, f'{stem}_test.parquet')\n","    else:\n","        return pd.concat([\n","            load_data(stock_id, stem, DataBlock.TRAIN),\n","            load_data(stock_id, stem, DataBlock.TEST)\n","        ]).reset_index(drop=True)\n","\n","def load_book(stock_id: int, block: DataBlock=DataBlock.TRAIN) -> pd.DataFrame:\n","    return load_data(stock_id, 'book', block)\n","\n","\n","def load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n","    return load_data(stock_id, 'trade', block)\n","\n","def calc_wap1(df: pd.DataFrame) -> pd.Series:\n","    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n","    return wap\n","\n","\n","def calc_wap2(df: pd.DataFrame) -> pd.Series:\n","    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n","    return wap\n","\n","\n","def realized_volatility(series):\n","    return np.sqrt(np.sum(series**2))\n","\n","\n","def log_return(series: np.ndarray):\n","    return np.log(series).diff()\n","\n","\n","def log_return_df2(series: np.ndarray):\n","    return np.log(series).diff(2)\n","\n","def flatten_name(prefix, src_names):\n","    ret = []\n","    for c in src_names:\n","        if c[0] in ['time_id', 'stock_id']:\n","            ret.append(c[0])\n","        else:\n","            ret.append('.'.join([prefix] + list(c)))\n","    return ret\n","\n","def make_book_feature(stock_id, block = DataBlock.TRAIN):\n","    book = load_book(stock_id, block)\n","\n","    book['wap1'] = calc_wap1(book)\n","    book['wap2'] = calc_wap2(book)\n","    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n","    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n","    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n","    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n","    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n","    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n","\n","    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n","    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n","    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n","    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n","    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n","    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n","    \n","    features = {\n","        'seconds_in_bucket': ['count'],\n","        'wap1': [np.sum, np.mean, np.std],\n","        'wap2': [np.sum, np.mean, np.std],\n","        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n","        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n","        'wap_balance': [np.sum, np.mean, np.std],\n","        'price_spread':[np.sum, np.mean, np.std],\n","        'bid_spread':[np.sum, np.mean, np.std],\n","        'ask_spread':[np.sum, np.mean, np.std],\n","        'total_volume':[np.sum, np.mean, np.std],\n","        'volume_imbalance':[np.sum, np.mean, np.std]\n","    }\n","    \n","    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n","    agg.columns = flatten_name('book', agg.columns)\n","    agg['stock_id'] = stock_id\n","    \n","    for time in [450, 300, 150]:\n","        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n","        d.columns = flatten_name(f'book_{time}', d.columns)\n","        agg = pd.merge(agg, d, on='time_id', how='left')\n","    return agg\n","\n","def make_trade_feature(stock_id, block = DataBlock.TRAIN):\n","    trade = load_trade(stock_id, block)\n","    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n","\n","    features = {\n","        'log_return':[realized_volatility],\n","        'seconds_in_bucket':['count'],\n","        'size':[np.sum],\n","        'order_count':[np.mean],\n","    }\n","\n","    agg = trade.groupby('time_id').agg(features).reset_index()\n","    agg.columns = flatten_name('trade', agg.columns)\n","    agg['stock_id'] = stock_id\n","        \n","    for time in [450, 300, 150]:\n","        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n","        d.columns = flatten_name(f'trade_{time}', d.columns)\n","        agg = pd.merge(agg, d, on='time_id', how='left')\n","    return agg\n","\n","def make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n","    book = load_book(stock_id, block)\n","\n","    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n","    time_ids = list(set(prices.index))\n","\n","    ticks = {}\n","    for tid in time_ids:\n","        try:\n","            price_list = prices.loc[tid].values.flatten()\n","            price_diff = sorted(np.diff(sorted(set(price_list))))\n","            ticks[tid] = price_diff[0]\n","        except Exception:\n","            print_trace(f'tid={tid}')\n","            ticks[tid] = np.nan\n","        \n","    dst = pd.DataFrame()\n","    dst['time_id'] = np.unique(book['time_id'])\n","    dst['stock_id'] = stock_id\n","    dst['tick_size'] = dst['time_id'].map(ticks)\n","\n","    return dst\n","\n","\n","def make_features(base, block):\n","    stock_ids = set(base['stock_id'])\n","    with timer('books'):\n","        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n","        book = pd.concat(books)\n","\n","    with timer('trades'):\n","        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n","        trade = pd.concat(trades)\n","\n","    with timer('extra features'):\n","        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n","        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n","        #df = make_extra_features(df)\n","\n","    return df\n","\n","\n","def make_features_v2(base, block):\n","    stock_ids = set(base['stock_id'])\n","    with timer('books(v2)'):\n","        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n","        book_v2 = pd.concat(books)\n","\n","    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n","    return d"]},{"cell_type":"code","execution_count":null,"id":"9e763504-67e4-4c17-81f5-6b24d5c68910","metadata":{"id":"9e763504-67e4-4c17-81f5-6b24d5c68910","outputId":"8d504ed8-5845-4240-ce22-2b921ae1c07c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[books]  24.425sec\n","[trades]  3.642sec\n","[extra features]  0.075sec\n"]}],"source":["df = make_features(train, DataBlock.TRAIN)"]},{"cell_type":"code","execution_count":null,"id":"facd0585-8118-4087-b92e-a51ffc536bc2","metadata":{"id":"facd0585-8118-4087-b92e-a51ffc536bc2"},"outputs":[],"source":["reg = lgbm.LGBMRegressor()\n","df['stock_id'] = df['stock_id'].astype(str)\n","dummies = pd.get_dummies(df['stock_id'])\n","dummies.index = df.index\n","df_enriched = pd.concat([df, dummies], axis = 1)\n","Y = df['target']\n","X = df_enriched.drop(['target', 'stock_id'], axis=1)"]},{"cell_type":"code","execution_count":null,"id":"3b8fa4fd-2c58-4f47-930e-0a958270e95d","metadata":{"id":"3b8fa4fd-2c58-4f47-930e-0a958270e95d","outputId":"d8e46e09-fa8d-495a-beef-ce8a6b23fdbe"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>book.wap1.sum</th>\n","      <th>book.wap2.sum</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>303.125061</td>\n","      <td>303.105539</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>200.047768</td>\n","      <td>200.041171</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>187.913849</td>\n","      <td>187.939824</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>119.859781</td>\n","      <td>119.835941</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>175.932865</td>\n","      <td>175.934256</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11485</th>\n","      <td>552.091492</td>\n","      <td>552.099365</td>\n","    </tr>\n","    <tr>\n","      <th>11486</th>\n","      <td>485.168854</td>\n","      <td>485.163422</td>\n","    </tr>\n","    <tr>\n","      <th>11487</th>\n","      <td>486.847137</td>\n","      <td>486.845856</td>\n","    </tr>\n","    <tr>\n","      <th>11488</th>\n","      <td>475.116730</td>\n","      <td>475.116455</td>\n","    </tr>\n","    <tr>\n","      <th>11489</th>\n","      <td>471.808990</td>\n","      <td>471.816162</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11490 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["       book.wap1.sum  book.wap2.sum\n","0         303.125061     303.105539\n","1         200.047768     200.041171\n","2         187.913849     187.939824\n","3         119.859781     119.835941\n","4         175.932865     175.934256\n","...              ...            ...\n","11485     552.091492     552.099365\n","11486     485.168854     485.163422\n","11487     486.847137     486.845856\n","11488     475.116730     475.116455\n","11489     471.808990     471.816162\n","\n","[11490 rows x 2 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["X = df[['book.wap1.sum', 'book.wap2.sum']]\n","X"]},{"cell_type":"code","execution_count":null,"id":"345a94d4-430c-4b74-8299-5c639663ce6f","metadata":{"id":"345a94d4-430c-4b74-8299-5c639663ce6f"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_eval, y_train, y_eval = train_test_split(X, Y, test_size=0.1)"]},{"cell_type":"code","execution_count":null,"id":"673e969e-727b-4bfe-b0a0-c6c41f911eb7","metadata":{"id":"673e969e-727b-4bfe-b0a0-c6c41f911eb7"},"outputs":[],"source":["reg = lgbm.LGBMRegressor(n_estimators=20)\n","eval_set = [(X_eval, y_eval)]\n","\n","reg.fit(\n","    X_train,\n","    y_train,\n","    eval_set=eval_set,\n","    eval_metric=\"mape\",\n",")"]},{"cell_type":"code","execution_count":null,"id":"c518c853-0756-4f66-b1d7-af7b982442d0","metadata":{"id":"c518c853-0756-4f66-b1d7-af7b982442d0"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"name":"LightGBM.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}